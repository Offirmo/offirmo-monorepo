??? https://github.com/SkalskiP/courses crème de la crème of AI courses
+++ 2023 https://9elements.com/blog/ai-glossary/
+++ https://platform.openai.com/docs/introduction/key-concepts
+++ official OpenAI doc https://platform.openai.com/docs/introduction
Agentic AI systems = AI systems that can pursue complex goals with limited direct supervision https://openai.com/research/practices-for-governing-agentic-ai-systems
agents -- principals
agents = you provide a goal, They'll generate a task list and get to work https://zapier.com/blog/ai-agent/
AGI -- ANI -> AGI -> ASI = narrow(weak) / general(strong) / super[human](strong)
AGI ~ strong AI = can independently learn new problem-solving strategies that are not explicitly included in its original model or training data
AI - Artificial intelligence = intentionally vague term / "solving tasks or problems by computers/machines, which require a form of human-like intelligence"
AI -- weak AI = task specific
AI 50 2023 https://www.forbes.com/lists/ai50/?sh=5f9503cc290f
AI winter
alignment -- generic = conform to predefined values, usually ethical or social values, Truthfulness, Harmlessness, Helpfulness
alignment -- super alignment =  https://openai.com/blog/introducing-superalignment  https://openai.com/blog/superalignment-fast-grants
alignment =  making AI do what its users want it to do (and nothing more) see also "paper clip"
annotation https://www.peopleforai.com/ https://www.isahit.com/
API -- Gemini https://ai.google.dev/docs/gemini_api_overview
assistant = entities capable of performing tasks for users. assistants operate based on the instructions in a context window + tools https://platform.openai.com/docs/introduction/assistants
attention
attention -- flash attention = ???
BERT
bias
BLOOM = "BigScience Large Open-science Open-access Multilingual Language Model" = deprecated former big model
BPE Byte Pair Encoding is a way of converting text into tokens
canny edge (generative AI) https://en.wikipedia.org/wiki/Canny_edge_detector
chain-of-thoughts https://arxiv.org/abs/2201.11903
chatbots = the reason why AI exploded in 2023
citation accuracy
common sense https://commonsense.run/
context
control vector https://vgel.me/posts/representation-engineering/
ControlNet = a type of model for controlling image diffusion models by conditioning the model with an additional input image https://huggingface.co/docs/diffusers/using-diffusers/controlnet https://github.com/lllyasviel/ControlNet
cross encoder https://www.sbert.net/examples/applications/cross-encoder/README.html
dataset  https://www.fatml.org/media/documents/datasheets_for_datasets.pdf
deep reinforcement learning (deep RL) course: https://spinningup.openai.com/
developer vs sociotechnic vs project organizer
diffusers
diffusion
Discrete Variational Auto Encoder (VAE) for image generation? https://medium.com/@jaswanth04/discrete-variational-auto-encoder-explained-41493ebe294d
DL Deep Learning (NN)
embeddings = raw underlying representation of a concept preserving aspects of its content and/or its meaning (Of course it's model-dependent?) https://platform.openai.com/docs/guides/embeddings/use-cases
fine-tuning
function calling (bad name) = connect large language models to external tools https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling https://platform.openai.com/docs/guides/function-calling
generative AI
GLUE language understanding benchmark
GPT "Generative Pre-trained Transformer" = understand language (natural & formal)
Grounded Generation = see RAG
Hallucination "Vectara's Hallucination Evaluation Model"
Hallucination Leaderboard https://github.com/vectara/hallucination-leaderboard
hallucinations "King Renoit" https://zapier.com/blog/ai-hallucinations/
HHH helpful, honest, and harmless
Hierarchical Navigable Small World (HNSW)
HNSW search algorithm
https://aleteia.org/tag/artificial-intelligence/
https://www.perspectiveapi.com/
https://www.sbert.net/
Hugging Face = "the GitHub of machine learning" = the collaboration platform for the machine learning community https://www.techtarget.com/whatis/definition/Hugging-Face
huggingface.co "space"
inference
inference = what happens when we prompt
JAX
langchain = a framework for developing applications powered by language models https://python.langchain.com/docs/get_started/introduction
latency
laziness = cases where the model doesn’t complete a task https://openai.com/blog/new-embedding-models-and-api-updates
learning -- Transfer learning = a technique where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task
lib -- PyTorch
lib -- Transformers
LLM -- "Language Model for Dialogue Applications" (LaMDA) 137B model from Google AI
LLM -- "Pathways Language Model" (PaLM) 540B model from Google AI
LLMs - large language models
ML - machine learning https://en.wikipedia.org/wiki/Machine_learning ~"subfield of AI that describes AI using data and algorithms without a programmer explicitly specifying the solution path through program code"
moat - We have no moat https://www.semianalysis.com/p/google-we-have-no-moat-and-neither
model -- efficiency
model -- Transformer model
model (NN) = sort of snapshot of a trained neural network / brain = NN (structure + parameters) https://huggingface.co/docs/hub/models
NLG Natural Language Generation
NLP Natural Language Processing
NN Neural Networks "computer architecture inspired by the human brain"
OpenAI GPTs = custom versions of ChatGPT that you can create for a specific purpose with instructions, expanded knowledge, and custom capabilities
parameter -- hyperparameter = values provided to the model from the outside to make adjustments that can influence the learning process and its performance, among other things
parameter (NN)
Pathways = broader AU architecture underpinning PaLM
Pathways Autoregressive Text-to-Image model (Parti), is an autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge.
player -- Google AI https://ai.google/
player -- https://modal.com/blog/embedding-wikipedia
player -- https://vectara.com/ = RAG
player -- https://www.adept.ai/
player -- Qdrant -- vector database https://qdrant.tech/blog/series-a-funding-round/
pre-training
Pytorch
ReAct flow
Representation Engineering = calculating a "control vector" that can be read from or added to model activations during inference to interpret or control the model's behavior https://vgel.me/posts/representation-engineering/
research directions -- honesty, chain-of-thought faithfulness, adversarial robustness, evals and testbeds...
research directions -- Interpretability: How can we understand model internals? And can we use this to e.g. build an AI lie detector?
research directions -- Scalable oversight: How can we use AI systems to assist humans in evaluating the outputs of other AI systems on complex tasks?
research directions -- Weak-to-strong generalization: Humans will be weak supervisors relative to superhuman models. Can we understand and control how strong models generalize from weak supervision?
Retrieval-Augmented Generation (RAG) = a methodology that assists Large Language Models (LLMs) generate accurate and up-to-date information https://medium.com/@rushing_andrei/building-a-basic-rag-retrieval-augmented-generation-system-in-a-rails-app-247ccce5d1d2
RLHF reinforcement learning from human feedback
SageMaker (AWS)
Searle's Chinese Room https://plato.stanford.edu/entries/chinese-room/
singularity
SMoE "sparse mixture of experts" model
sociotechnic = skilled at analyzing the interaction of technology and society long-term: lawyers, ethicists, sociologists, or rights advocates
Stable Diffusion
superintelligence "AI vastly smarter than humans" https://openai.com/research/weak-to-strong-generalization
Support Vector Machines
symbolic logic
synthesizer = sound generation
T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales.
tasks -- vision -- Depth Estimation
tasks -- vision -- Image Classification
tasks -- vision -- Image Segmentation
tasks -- vision -- Image-to-Image
tasks -- vision -- Mask Generation
tasks -- vision -- Object Detection
tasks -- vision -- Unconditional Image Generation
tasks -- vision -- Video Classification
tasks -- vision -- Zero-Shot Image Classification
tasks -- vision -- Zero-Shot Object Detection
temperature
tensor ??
TensorFlow = outdated, this is all pyTorch now. See also JAX
Text generation models -> see GPT https://platform.openai.com/docs/introduction/text-generation-models
tokenization https://platform.openai.com/tokenizer vs https://github.com/huggingface/tokenizers
tokenizer -- sentencepiece https://github.com/google/sentencepiece
tokenizer -- tiktoken https://tiktokenizer.vercel.app/
tokenizer = good explanation https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c
tokens https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
training (NN) -- Overfitting
training (NN) -- Reinforcement Learning
training (NN) -- supervised learning
training (NN) -- transfer learning = see fine-tuning
training (NN) -- unsupervised learning
training (NN) = iterative process in which data is passed to a neural network + parameters of the neural network are adjusted to achieve an optimal solution for the given problem. a dataset is required
Trainium
transfer learning
transformers "a new type of NLP model that demolished the reading comprehension abilities of both humans and the best AI incumbent at the time" ~pattern recognition techniques
transformers (hugging face) = a magic python library that can auto-download models on demand https://github.com/huggingface/transformers BUT it's for research, not prod (no unified API for ex.)
Turing test
vector -- difference with embedding??
vector database
vector database https://weaviate.io/
vector database https://www.pinecone.io
weak-to-strong generalization https://openai.com/research/weak-to-strong-generalization
