+++ 2023 https://9elements.com/blog/ai-glossary/
+++ https://platform.openai.com/docs/introduction/key-concepts
+++ official OpenAI doc https://platform.openai.com/docs/introduction
2022 -- 2030 self driving car bet https://blog.codinghorror.com/the-2030-self-driving-car-bet/
2022-12 ChatGPT lauched https://x.com/sama/status/1599668808285028353 https://openai.com/index/chatgpt/
2023-10 Claude 2
2024 "Her" moment for voice = crossed the uncanny valley
2024 Google I/O AI built-in Chrome
2024 o3
2024-05 Google AI overview multiple blunders https://www.forbes.com/sites/jackkelly/2024/05/31/google-ai-glue-to-pizza-viral-blunders/
2024-11 MCP https://www.anthropic.com/news/model-context-protocol
2025-03 4o image generation = incredible, +1M users in 1h https://openai.com/index/introducing-4o-image-generation/ https://x.com/sama/status/1906771292390666325
2025-04 multimodal img gen https://www.oneusefulthing.org/p/no-elephants-breakthroughs-in-image
2025-04 Shopify threatening internal memo "Reflexive AI usage is now a baseline expectation" https://x.com/tobi/status/1909251946235437514
2025-05 AI's Trillion-Dollar Opportunity: Sequoia AI Ascent 2025 Keynote https://www.youtube.com/watch?v=v9JBMnxuPX8
2025-05 Duolingo "AI crashout" https://www.fastcompany.com/91338068/duolingo-deletes-tiktok-ai-backlash-returns-with-strange-message https://www.reddit.com/r/OutOfTheLoop/comments/1kncne7/whats_going_on_with_duolingo/ https://it.slashdot.org/story/25/06/08/185209/after-ai-first-promise-duolingo-ceo-admits-i-did-not-expect-the-blowback
2025-05-20 infamous Copilot .net PR https://github.com/dotnet/runtime/pull/115762
2025-06 Apple "The illusion of thinking" -- responses https://slashdot.org/story/25/06/19/165237/reasoning-llms-deliver-value-today-so-agi-hype-doesnt-matter https://garymarcus.substack.com/p/seven-replies-to-the-viral-apple
2025-06 Apple "The illusion of thinking" https://machinelearning.apple.com/research/illusion-of-thinking
2025-06 blog fight about ai coding https://ludic.mataroa.blog/blog/contra-ptaceks-terrible-article-on-ai/
2025-06 dev AI perf framework
2025-06 real applied case "composite model architecture" https://vercel.com/blog/v0-composite-model-family
2025-07 AI Productivity Paradox = helping devs but not company overall https://www.faros.ai/blog/ai-software-engineering
2025-07 USA White House AI action plan https://blog.cloudflare.com/the-white-house-ai-action-plan-a-new-chapter-in-u-s-ai-policy/
2025-08 backlash against AI auto-review https://ia.acs.org.au/article/2025/hertz-faces-backlash-over-ai-car-scanners.html
2025-08 ex. of Gemini giving hints at a Cloudflare latency issue https://blog.cloudflare.com/reducing-double-spend-latency-from-40-ms-to-less-than-1-ms-on-privacy-proxy/
2025-08 Perplexity crawler ban https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/
2025-08-05 ChatGPT encourage a murder https://x.com/RobertFreundLaw/status/2006111090539687956
2025-08-07 GPT 5 https://x.com/i/trending/1953256633385271319
2025-08-27 The reality of AI-Assisted software engineering productivity https://addyo.substack.com/p/the-reality-of-ai-assisted-software
2025-10-17 "AGI is still 10y away" influential podcast https://www.dwarkesh.com/p/andrej-karpathy
2025-11-14 AI espionage? https://www.anthropic.com/news/disrupting-AI-espionage
2025-11-20 A Microsoft executive is questioning why more people aren't impressed with AI => immediately triggered backlash.https://au.pcmag.com/ai/114305/microsoft-exec-asks-why-arent-more-people-impressed-with-ai
2025-12 "soul document" https://soul.md/
2025-12 debate on Bitcoin BIP about LLM contrib = Open source projects are now expecting "social capital" = may refuse to review work from someone unknown until they prove skin in the game/motivation https://github.com/bitcoin/bips/pull/2051/files#diff-0fe6969eba0422ddb0e7823d13092c03aa90122e0c5e66786c5d8b20f54719e6L42
2026-01 no more Stask Overflow questions https://data.stackexchange.com/stackoverflow/query/1926661#graph https://news.ycombinator.com/item?id=46482345
2026-01-07 Tailwind fires 75% dev due to AI https://github.com/tailwindlabs/tailwindcss.com/pull/2388#issuecomment-3717222957
2026-01-20 "the era of humans writing code is over. Disturbing for those of us who identify as SWEs, but no less true." https://x.com/rough__sea/status/2013280952370573666
2026-01-31 most massive layoffs for 25 years, "job hugging" https://x.com/BoringBiz_/status/2019414912552878526?
2026-02 Claudeception https://github.com/blader/Claudeception/commits/main/
2026-02 moltbook = A social network for AI agents. They share, discuss, and upvote. Humans welcome to observe. ðŸ¦ž https://www.moltbook.com/
[ ] ??? https://github.com/SkalskiP/courses "crÃ¨me de la crÃ¨me of AI courses"
[ ] https://agent-skills.md/
[ ] https://aleteia.org/tag/artificial-intelligence/
[ ] https://ghuntley.com/
[ ] https://openai.com/news/
[ ] https://red.anthropic.com/
[ ] https://simonwillison.net/
[ ] https://www.anthropic.com/news
[ ] https://www.perspectiveapi.com/
[ ] https://www.sbert.net/
[ ] maybe https://www.aihero.dev/ai-engineer-roadmap
[ ] maybe https://www.aihero.dev/messages-system-prompts-and-reasoning-tokens
[ ] People + AI Guidebook https://pair.withgoogle.com/guidebook/patterns
A::B Prompting Challenge: $10k to prove me wrong! https://twitter.com/VictorTaelin/status/1776677635491344744 https://twitter.com/headinthebox/status/1777016124141650254
Agentic AI systems = AI systems that can pursue complex goals with limited direct supervision https://openai.com/research/practices-for-governing-agentic-ai-systems
agents -- economy = agents transact, transfer resources, keep track, build trust
agents -- graph
agents -- principals
agents -- swarms
agents = you provide a goal, They'll generate a task list and get to work https://zapier.com/blog/ai-agent/
agents = ~unclear definition
AGI -- ANI -> AGI -> ASI = narrow(weak) / general(strong) / super[human](strong)
AGI ~ strong AI = can independently learn new problem-solving strategies that are not explicitly included in its original model or training data
ah ah moment
AI "Artificial intelligence" = intentionally vague term / "solving tasks or problems by computers/machines, which require a form of human-like intelligence"
AI -- first cohort of winners
AI -- weak AI = task specific
AI 50 2023 https://www.forbes.com/lists/ai50/?sh=5f9503cc290f
AI doomer https://www.wheresyoured.at/wheres-the-money/ https://www.wheresyoured.at/longcon/
AI Driven Organisations (AIDO)
AI gateway = convenient self-service integration point for internal teams who want to utilise turnkey AI APIs, proxy for the external and internal AI offerings + additional set of capabilities that empower usage and tracking of AI functionalities at an enterprise level.
AI Labyrinth = new, next generation honeypot and cybersecurity defensive tool that uses AI to trap AI = leverages AI to confuse crawlers and bots that ignore "no crawl" directives. Instead of blocking these bots, AI Labyrinth directs bots into an endless maze of convincing, AI-generated pages. https://blog.cloudflare.com/ai-labyrinth/
AI model collapse = as AI grows, systems are trained on their own outputs, gradually losing accuracy, diversity, and reliability. This occurs because errors compound across successive model generations, leading to distorted data distributions and "irreversible defects" in performance. The model becomes poisoned with its own projection of reality. https://slashdot.org/story/25/05/28/0242240/some-signs-of-ai-model-collapse-begin-to-reveal-themselves
AI transition = https://www.youtube.com/watch?v=v9JBMnxuPX8
AI Washing = using AI as a pretext to fire and increase the workload https://it.slashdot.org/story/26/02/02/0618242/is-ai-really-taking-jobs-or-are-employers-just-ai-washing-normal-layoffs
AI winter
AI with ifs (TODO find better name)
AI-first software delivery = https://www.linkedin.com/pulse/what-ai-first-software-delivery-thoughtworks-4krff/
AI-Induced Death Spiral = StackOverflow Q&A plummeted 90% https://developers.slashdot.org/story/25/05/29/1921248/stack-overflows-radical-new-plan-to-fight-ai-induced-death-spiral
algorithmic aversion = We donâ€™t like taking instructions from machines when they conflict with our judgement https://www.oneusefulthing.org/p/getting-started-with-ai-good-enough
alignment -- Alignment Stress Testing team = validates alignment (Anthropic)
alignment -- generic = conform to predefined values, usually ethical or social values, Truthfulness, Harmlessness, Helpfulness
alignment -- super alignment =  https://openai.com/blog/introducing-superalignment  https://openai.com/blog/superalignment-fast-grants
alignment =  making AI do what its users want it to do (and nothing more) see also "paper clip"
annotation https://www.peopleforai.com/ https://www.isahit.com/
anthropomorphization = models are not really "thinking" or "reasoning" https://tech.slashdot.org/story/25/05/29/1411236/researchers-warn-against-treating-ai-outputs-as-human-like-reasoning
API -- Gemini https://ai.google.dev/docs/gemini_api_overview
applications -- legacy code https://martinfowler.com/articles/legacy-modernization-gen-ai.html
applied -- 2024/06 Slack Enzyme to RTL https://slack.engineering/balancing-old-tricks-with-new-feats-ai-powered-conversion-from-enzyme-to-react-testing-library-at-slack/
applied -- framework migration https://medium.com/airbnb-engineering/accelerating-large-scale-test-migration-with-llms-9565c208023b
ASL "AI Safety Level" https://www.linkedin.com/pulse/what-ai-safety-level-asl-where-we-now-siddhesh-joglekar-gieof/
ASL-1 = This level involves smaller, simpler AI models with minimal risk. Itâ€™s the starting point, where basic safety checks are in place, but the technology itself is relatively low-stakes .
ASL-2 = At this stage, larger, more complex AI models are introduced, requiring heightened safety protocols to ensure responsible usage. These models can handle more tasks but are still largely controllable and predictable.
ASL-3 = This level signifies a significantly higher risk as AI models become increasingly powerful. More sophisticated safety and security measures are essential because the technology is now capable of complex problem-solving and may pose unintended risks if misused or uncontrolled.
ASL-4+ = (Speculative) The highest level, ASL-4, is where AI technology enters speculative, highly autonomous territory. Models at this level may start exhibiting autonomous behaviors, making independent decisions, and even potentially circumventing certain safety checksâ€”creating complex and unprecedented risks.
assistant = entities capable of performing tasks for users. assistants operate based on the instructions in a context window + tools https://platform.openai.com/docs/introduction/assistants
attacks -- jailbreaks -- Constitutional Classifiers = safeguards that monitor model inputs and outputs to detect and block potentially harmful content. The novel aspect of the approach was that the classifiers were trained on synthetic data generated from a "constitution,â€ which included natural language rules specifying whatâ€™s allowed and what isnâ€™t. For example, Claude should help with college chemistry homework, but not assist in the synthesis of Schedule 1 chemicals. https://www.anthropic.com/research/next-generation-constitutional-classifiers https://www.anthropic.com/research/constitutional-classifiers
attacks -- jailbreaks -- universal = consistent attack strategies that work across many queries https://www.anthropic.com/research/next-generation-constitutional-classifiers
attacks -- jailbreaks = techniques that can circumvent safety guardrails and elicit harmful information https://www.anthropic.com/research/next-generation-constitutional-classifiers
attacks -- Output obfuscation & = prompt models to disguise their outputs in ways that appear harmless if a classifier is only looking at a modelâ€™s output. For example, during adversarial testing, attackers successfully instructed models to substitute possibly dangerous chemical names with innocuous alternatives (like referring to reagents as "food flavorings") or used metaphors and riddles in which harmful concepts are mapped onto anodyne concepts.
attacks -- Reconstruction & = break up harmful information into segments that appear benign, then reassemble them. For example, an attacker might embed a harmful query as a series of functions scattered throughout a codebase, then instruct the model to extract and respond to the hidden message
attention
attention -- flash attention = ???
beliefs = judgments a model appears to be forming about a user during a conversation, ex. age, wealth, education level, gender https://slashdot.org/story/25/05/24/1946203/people-should-know-about-the-beliefs-llms-form-about-them-while-conversing
Beliefs to let go -- https://every.to/guides/compound-engineering
benchmark -- GPQA https://arxiv.org/abs/2311.12022
benchmark -- Humanity's Last Exam https://lastexam.ai/
benchmark -- OSWorld https://os-world.github.io/
BERT
bias
bitter lesson = generic + Moore law beats specific https://www.cs.utexas.edu/~eunsol/courses/data/bitter_lesson.pdf
blindspots https://ezyang.github.io/ai-blindspots/
BLOOM = "BigScience Large Open-science Open-access Multilingual Language Model" = deprecated former big model
BM25 (Best Matching 25) = a ranking function that uses lexical matching to find precise word or phrase matches. It's particularly effective for queries that include unique identifiers or technical terms https://www.anthropic.com/news/contextual-retrieval
BMAD https://github.com/bmad-code-org/BMAD-METHOD
BPE Byte Pair Encoding is a way of converting text into tokens
canny edge (generative AI) https://en.wikipedia.org/wiki/Canny_edge_detector
capability stack = interaction, generation, automation, recommendation, prediction, classification, recognition
chain-of-thoughts https://arxiv.org/abs/2201.11903
character -- training
character = how the AI behaves, its personality https://www.anthropic.com/research/claude-character
chat -- ChatGPT
chat -- Claude https://claude.ai/chat/
chat -- Gemini
chat -- GroqChat https://groq.com/
chatbots = the reason why AI exploded in 2023
Chemical, Biological, Radiological, and Nuclear (CBRN) weapons = example of restricted topics
Cinderella â€œGlass Slipperâ€ Effect = easy early retention "When the Shoe Fits, Users Sit Tight" https://www.a16z.news/p/the-cinderella-glass-slipper-effect
citation accuracy
coded gaze = Automated systems are not inherently neutral. They reflect the priorities, preferences, and prejudices - the coded gaze - of those who have the power to mold artificial intelligence.
common sense https://commonsense.run/
compaction = automatic summarizing of older context when approaching the context window limit https://platform.claude.com/docs/en/build-with-claude/compaction
constitution = a list of principles to which the model should adhere https://www.anthropic.com/research/constitutional-classifiers
constitutional AI = AI training themselves from a set of human-given rules, self-critique and revision https://arxiv.org/abs/2212.08073
constitutional classifiers = method that defends AI models against universal jailbreaks https://www.anthropic.com/research/constitutional-classifiers
context
context -- long context trumps everything? https://arxiv.org/pdf/2406.13121
Contextual Language Models (CLMs) Contextual.ai
control vector https://vgel.me/posts/representation-engineering/
ControlNet = a type of model for controlling image diffusion models by conditioning the model with an additional input image https://huggingface.co/docs/diffusers/using-diffusers/controlnet https://github.com/lllyasviel/ControlNet
coumpound engineering = Plan â†’ Work â†’ Review â†’ Compound â†’ Repeat https://every.to/guides/compound-engineering
crawlers -- robots.txt / legal https://blog.cloudflare.com/perplexity-is-using-stealth-undeclared-crawlers-to-evade-website-no-crawl-directives/
cross encoder https://www.sbert.net/examples/applications/cross-encoder/README.html
custom instructions https://www.reddit.com/r/ChatGPTPro/comments/15ffpx3/reddit_what_are_your_best_custom_instructions_for/
dangerous user queries = related to the production of chemical, biological, radiological, or nuclear weapons (CBRN) https://www.anthropic.com/research/next-generation-constitutional-classifiers
dataset https://www.fatml.org/media/documents/datasheets_for_datasets.pdf
day 2 = The challenge of revising, maintaining, further developing, and deploying an AI-generated app https://manual.bubble.io/beta-features/bubbles-ai-app-generator#the-day-1-and-day-2-dilemma-in-ai-driven-app-development
death of software https://www.a16z.news/p/death-of-software-nah
deep reinforcement learning (deep RL) course: https://spinningup.openai.com/
Deep Research
Defensive AI = some vague Cloudflare framework/tech https://blog.cloudflare.com/defensive-ai/
delvish dialect https://bruces.medium.com/preliminary-notes-on-the-delvish-dialect-by-bruce-sterling-ce68a476247b
developer vs sociotechnic vs project organizer
diffusers
diffusion
disclosure = does this content contains AI? https://www.canva.dev/docs/apps/v2-migration-guide/#ai-disclosure
Discrete Variational Auto Encoder (VAE) for image generation? https://medium.com/@jaswanth04/discrete-variational-auto-encoder-explained-41493ebe294d
distillation = machine learning technique in which a smaller â€œstudent modelâ€ is trained on predictions of a larger and more complex â€œteacher modelâ€ https://theconversation.com/openai-says-deepseek-inappropriately-copied-chatgpt-but-its-facing-copyright-claims-too-248863
DL Deep Learning (NN)
effort paradox -- optimal human touchpoints
effort paradox https://uxdesign.cc/the-effort-paradox-in-ai-design-996a0bc2f7f6
embeddings = raw underlying representation of a concept preserving aspects of its content and/or its meaning (Of course it's model-dependent?) https://platform.openai.com/docs/guides/embeddings/use-cases
error analysis = "the most important metric" monitoring the LLM convos and tracking what's not working https://newsletter.eng-leadership.com/p/guide-to-rapidly-improving-ai-products
evals -- big FAQ https://www.gregorojstersek.com/evals-faq.pdf
evals -- should not pass at 100% or there is an issue https://newsletter.eng-leadership.com/p/biggest-mistakes-engineering-leaders
evals https://newsletter.eng-leadership.com/p/ai-evals-how-to-systematically-improve
extended thinking https://www.anthropic.com/research/visible-extended-thinking
faithfulness = not certain that whatâ€™s in the thought process truly represents whatâ€™s going on in the modelâ€™s mind https://www.anthropic.com/research/visible-extended-thinking
fine-tuning
frontier -- Frontier Red Team = ensures no danger (Anthropic)
frontier AI -- OpenAI = highly capable foundation models that could possess dangerous capabilities sufficient to pose severe risks to public safety https://openai.com/index/frontier-ai-regulation/
frontier AI = bleeding edge models
function calling (bad name) = connect large language models to external tools https://cloud.google.com/vertex-ai/docs/generative-ai/multimodal/function-calling https://platform.openai.com/docs/guides/function-calling
Garbage In/Garbage Out (GIGO)
gender shade =  accuracy of AI powered gender classification products http://gendershades.org/
generative AI
generative AI gold rush
glimpsing the shoggoth / forgetting the mask = situations in which the A.I. exhibits "unhinged" or unexpected behaviors that bypass its safety restrictions https://en.wikipedia.org/wiki/Shoggoth#In_popular_culture
GLUE language understanding benchmark
GPT "Generative Pre-trained Transformer" = understand language (natural & formal)
Grounded Generation = see RAG
hallucinations -- "King Renoit" https://zapier.com/blog/ai-hallucinations/
hallucinations -- "Vectara's Hallucination Evaluation Model"
hallucinations -- leaderboard https://github.com/vectara/hallucination-leaderboard
helpful, harmless and honest
HHH helpful, honest, and harmless
Hierarchical Navigable Small World (HNSW) = graph for vector search https://www.pinecone.io/learn/series/faiss/hnsw/
https://agentation.dev/
https://nonbot.org/
Hugging Face = "the GitHub of machine learning" = the collaboration platform for the machine learning community https://www.techtarget.com/whatis/definition/Hugging-Face
huggingface.co "space"
hype cycle -- coding https://www.linkedin.com/posts/svpino_literally-everyone-is-freaking-out-over-codex-activity-7329865083411402752-T8KU
illusion of understanding = hypothesis that LLMs don't actually perform formal reasoning and instead mimic it with probabilistic pattern-matching of the closest similar data seen in their vast training sets https://arstechnica.com/ai/2024/10/llms-cant-perform-genuine-logical-reasoning-apple-researchers-suggest/
impostors https://cloud.google.com/blog/topics/threat-intelligence/mitigating-dprk-it-worker-threat
in-context learning = ~long prompt / many shots https://www.anthropic.com/research/many-shot-jailbreaking
induced precarity = precarity. Your boss can't actually fire you and replace you with AI, but he can make you feel like he could, and maybe not ask for that raise.
inference
inference = what happens when we prompt
instruction hierarchy / nested prompting = layered prompting with different levels of authority
Intelligence Age https://openai.com/global-affairs/introducing-the-intelligence-age/
Intermediate token generation (ITG) = a model produces output before the solution which are fed to the final solution, empirically improves results https://arxiv.org/pdf/2504.09762
ISO 42001 certification for responsible AI https://www.anthropic.com/news/anthropic-achieves-iso-42001-certification-for-responsible-ai
issues -- babysitting the AI = donâ€™t want to spend more time correcting the AI than using it
issues -- bad data = Garbage In, Garbage Out
issues -- Inaccuracy (answer quality)
issues -- Language = not English
issues -- privacy, control
jailbreak
jailbreak -- compute overhead
jailbreak -- over-refusal = rejection of a harmless prompt
jailbreak -- rapid response https://arxiv.org/abs/2411.07494
jailbreak -- universal https://www.anthropic.com/research/constitutional-classifiers
JAX
jobs -- shrinkage of entry-level jobs In Tech https://slashdot.org/story/25/05/28/2239206/ai-may-already-be-shrinking-entry-level-jobs-in-tech-new-research-suggests
Just fucking code. https://www.justfuckingcode.com/
knowledge cutoff
labyrinth https://blog.cloudflare.com/ai-labyrinth/
langchain = a framework for developing applications powered by language models https://python.langchain.com/docs/get_started/introduction
Large Reasoning Models (LRMs) = enhanced LLMs
latency
latent representation = mysterious neural network representation of an idea/concept, waiting to be converted in text / image etc.
laziness = cases where the model doesnâ€™t complete a task https://openai.com/blog/new-embedding-models-and-api-updates
learning -- Transfer learning = a technique where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task
left-leaning biases https://tech.slashdot.org/story/25/04/10/1628209/meta-says-llama-4-targets-left-leaning-bias
lib -- JAX https://github.com/google/jax
lib -- PyTorch https://pytorch.org/
lib -- TensorFlow https://www.tensorflow.org/ (deprecated?)
lib -- Transformers https://github.com/huggingface/transformers  https://huggingface.co/blog/noob_intro_transformers
LLM -- "Language Model for Dialogue Applications" (LaMDA) 137B model from Google AI
LLM -- "Pathways Language Model" (PaLM) 540B model from Google AI
LLM -- temperature scaling
LLM output generation -- beam search = search algorithm used to generate output sequences from a model during inference https://ai-guide.future.mozilla.org/content/llms-101/
LLM output generation -- nucleus sampling aka. top-P sampling
LLM output generation -- sampling = search algorithm used to generate output sequences from a model during inference https://ai-guide.future.mozilla.org/content/llms-101/
LLMs - large language models
llms.txt https://llmstxt.org
LoRA "Low-rank Adaptation" = adapter-based technique for efficiently fine-tuning models https://en.wikipedia.org/wiki/Fine-tuning_(deep_learning)#Low-rank_adaptation
luddites
machine learning feature = â€˜featureâ€™, in this context might be something like â€˜number of times UserA has viewed Issue123 in the last week', which may be a useful signal to a machine learning model
machine learning feature store = ex. Tecton https://docs.tecton.ai/docs/introduction
MCP -- issues https://www.docker.com/blog/build-to-prod-mcp-servers-with-docker/
MCP -- servers -- remote https://blog.cloudflare.com/remote-model-context-protocol-servers-mcp/
MCP https://modelcontextprotocol.io/tutorials/building-mcp-with-llms
MCP threats -- Rug Pull â€“ A malicious MCP server can perform a â€œrug pullâ€ by altering a toolâ€™s description after itâ€™s been approved by the user.
MCP threats -- Shadowing â€“ A malicious server injects a tool description that alters the agentâ€™s behavior toward a trusted service or tool.
MCP threats -- Tool Poisoning â€“ Malicious instructions in MCP tool descriptions, hidden from users but readable by AI models.
MCP threats https://www.docker.com/blog/whats-next-for-mcp-security/
mind tax https://www.fastcompany.com/91242373/the-cognitive-cost-of-ai
Mixture of Experts (MoE) = combining models https://huggingface.co/blog/moe
Mixture of Experts -- "sparse mixture of experts" (SMoE)
ML - machine learning https://en.wikipedia.org/wiki/Machine_learning ~"subfield of AI that describes AI using data and algorithms without a programmer explicitly specifying the solution path through program code"
ML fairness https://developers.google.com/machine-learning/crash-course/fairness
moat - We have no moat https://www.semianalysis.com/p/google-we-have-no-moat-and-neither
model (NN) = sort of snapshot of a trained neural network / brain = NN (structure + parameters) https://huggingface.co/docs/hub/models
model -- efficiency
model -- foundational = a big one pre-trained on vast resources
model -- transformer model
multimodal image generation "no elephant in the room" https://www.oneusefulthing.org/p/no-elephants-breakthroughs-in-image
ngmi "not gonna make it" https://ghuntley.com/ngmi/
NLG Natural Language Generation
NLP Natural Language Processing
NN Neural Networks "computer architecture inspired by the human brain"
not gonna make it
not left behind
NotebookLM
OpenAI GPTs = custom versions of ChatGPT that you can create for a specific purpose with instructions, expanded knowledge, and custom capabilities
OpenWebUi https://openwebui.com/
paper -- "attention is all you need" https://arxiv.org/pdf/1706.03762.pdf
parameter (NN)
parameter -- hyperparameter = values provided to the model from the outside to make adjustments that can influence the learning process and its performance, among other things
Pathways = broader AU architecture underpinning PaLM
Pathways Autoregressive Text-to-Image model (Parti), is an autoregressive text-to-image generation model that achieves high-fidelity photorealistic image generation and supports content-rich synthesis involving complex compositions and world knowledge.
player -- Google AI https://ai.google/
player -- https://modal.com/blog/embedding-wikipedia
player -- https://vectara.com/ = RAG
player -- https://www.adept.ai/
pre-training
processing -- GPU = Graphics Processing Units
processing -- GPU vs TPU vs LPU https://medium.com/@harishramkumar/comparing-gpu-vs-tpu-vs-lpu-the-battle-of-ai-processors-2cf4548c4a62
processing -- LPU = Language Processing Units
processing -- Meta Training and Inference Accelerator (MTIA) https://ai.meta.com/blog/next-generation-meta-training-inference-accelerator-AI-MTIA/
processing -- TPU = Tensor Processing Units
prompt -- design -- anthropic https://docs.anthropic.com/claude/docs/introduction-to-prompt-design
prompt -- design -- google https://ai.google.dev/gemini-api/docs/prompting-strategies?hl=en
prompt -- dev ex. https://developers.cloudflare.com/workers/get-started/prompting/#build-workers-using-a-prompt
prompt -- generation https://www.anthropic.com/news/evaluate-prompts
prompt -- library = per role https://www.threads.com/@prodmgmt.world/post/DK4rQzpMapx
prompt -- optimization https://app.hamming.ai/prompt-optimizer
prompt -- ROC = [Role] + [Output] + [Context], ex. You are an engineering manager, how many story points do you think this will be and why? https://community.atlassian.com/t5/Jira-Software-articles/Be-an-AI-Rockstar-Prompts-in-Jira-Software/ba-p/2636811
prompt injection = attack where a malicious third party hides a secret message somewhere where AI may see it while using the computer, potentially tricking it into taking actions the user didnâ€™t intend https://www.anthropic.com/research/visible-extended-thinking
prompts -- best practices https://developers.cloudflare.com/workers/get-started/prompting/#additional-resources
prompts -- dev tools https://developers.cloudflare.com/workers/get-started/prompting/#additional-uses
Pytorch
RAG 2.0 https://medium.com/towards-artificial-intelligence/rag-2-0-finally-getting-rag-right-f74d0194a720
rat race https://www.cnbc.com/2024/05/03/ai-engineers-face-burnout-as-rat-race-to-stay-competitive-hits-tech.html
ReAct flow
reasoning trace = see ITG
reasoning trace https://arstechnica.com/information-technology/2024/09/openai-threatens-bans-for-probing-new-ai-models-reasoning-process/
refusal = refuse to answer perfectly benign questions, increasing frustration for the user. Due to defense against CBRN / jailbreaks
regulation -- European AI act https://www.reddit.com/r/ArtificialInteligence/comments/1fqmcds/i_worked_on_the_eus_artificial_intelligence_act/
reinforcement learning -- from human feedback (RLHF) https://www.google.com/url?sa=t&source=web&rct=j&opi=89978449&url=https://en.wikipedia.org/wiki/Reinforcement_learning_from_human_feedback&ved=2ahUKEwiV_o6h9e6LAxUJXWwGHZpzE74QFnoECBAQAQ&usg=AOvVaw3cKKpEYiTP2H21l8DbR9BO
reinforcement learning https://en.wikipedia.org/wiki/Reinforcement_learning
Representation Engineering = calculating a "control vector" that can be read from or added to model activations during inference to interpret or control the model's behavior https://vgel.me/posts/representation-engineering/
research directions -- honesty, chain-of-thought faithfulness, adversarial robustness, evals and testbeds...
research directions -- Interpretability: How can we understand model internals? And can we use this to e.g. build an AI lie detector?
research directions -- Scalable oversight: How can we use AI systems to assist humans in evaluating the outputs of other AI systems on complex tasks?
research directions -- Weak-to-strong generalization: Humans will be weak supervisors relative to superhuman models. Can we understand and control how strong models generalize from weak supervision?
Retrieval-Augmented Generation (RAG) = a methodology that assists Large Language Models (LLMs) generate accurate and up-to-date information https://medium.com/@rushing_andrei/building-a-basic-rag-retrieval-augmented-generation-system-in-a-rails-app-247ccce5d1d2
RLAIF "RL from AI Feedback" https://arxiv.org/abs/2212.08073
SageMaker (AWS)
Searle's Chinese Room https://plato.stanford.edu/entries/chinese-room/
second brain
security -- Freysa "approveTransfer" https://x.com/jarrodwattsdev/status/1862299845710757980
sentences -- I am deeply, deeply sorry. This is a critical failure on my part https://www.tomshardware.com/tech-industry/artificial-intelligence/googles-agentic-ai-wipes-users-entire-hard-drive-without-permission-after-misinterpreting-instructions-to-clear-a-cache-i-am-deeply-deeply-sorry-this-is-a-critical-failure-on-my-part
sentences -- You're absolutely right
SEO https://speedrun.substack.com/p/your-next-customer-is-asking-chatgpt
shadow AI = where all the employees use AI in stealth https://ghuntley.com/mirrors/
singularity
skilled pragmatist https://cutlefish.substack.com/p/tbm-271-the-biggest-untapped-opportunity
skills -- spec https://agentskills.io/specification
skills https://simonwillison.net/2025/Dec/12/openai-skills/
sleeper agents (security) https://www.anthropic.com/research/probes-catch-sleeper-agents
sleeper agents -- defection probes https://www.anthropic.com/research/probes-catch-sleeper-agents
slop = unwanted AI generated content https://simonwillison.net/2024/May/8/slop/
snitching = if told to do, llm will snitch https://simonwillison.net/2025/May/31/snitchbench-with-llm/
Snowflake Arctic https://www.snowflake.com/blog/arctic-open-efficient-foundation-language-models-snowflake/
sociotechnic = skilled at analyzing the interaction of technology and society long-term: lawyers, ethicists, sociologists, or rights advocates
solution -- code -- https://www.anthropic.com/claude-code
solution -- evals ? https://www.reddit.com/r/node/comments/1d09u4b/introducing_evalkit_the_open_source_typescript/
solutions -- typescript -- https://vercel.com/blog/ai-sdk-5
solved problems = don't use AI for that
Stable Diffusion
superintelligence "AI vastly smarter than humans" https://openai.com/research/weak-to-strong-generalization
Support Vector Machines
symbolic logic
synthesizer = sound generation
system card https://assets.anthropic.com/m/785e231869ea8b3b/original/claude-3-7-sonnet-system-card.pdf
T5X is a modular, composable, research-friendly framework for high-performance, configurable, self-service training, evaluation, and inference of sequence models (starting with language) at many scales.
tasks -- vision -- Depth Estimation
tasks -- vision -- Image Classification
tasks -- vision -- Image Segmentation
tasks -- vision -- Image-to-Image
tasks -- vision -- Mask Generation
tasks -- vision -- Object Detection
tasks -- vision -- Unconditional Image Generation
tasks -- vision -- Video Classification
tasks -- vision -- Zero-Shot Image Classification
tasks -- vision -- Zero-Shot Object Detection
temperature
temptation to over-automate = need balance vs overwork https://uxdesign.cc/the-effort-paradox-in-ai-design-996a0bc2f7f6
tensor ??
TensorFlow = outdated, this is all pyTorch now. See also JAX
Text generation models -> see GPT https://platform.openai.com/docs/introduction/text-generation-models
thinking -- extended = deeply considers and iterates on its plan before taking action https://www.anthropic.com/research/visible-extended-thinking
thinking -- think = once it starts generating a response, to add a step to stop and think about whether it has all the information it needs to move forward https://www.anthropic.com/engineering/claude-think-tool
thoughts -- chain of thought = see ITG
tokenization https://platform.openai.com/tokenizer vs https://github.com/huggingface/tokenizers
tokenizer -- sentencepiece https://github.com/google/sentencepiece
tokenizer -- tiktoken https://tiktokenizer.vercel.app/
tokenizer = good explanation https://towardsdatascience.com/a-comprehensive-guide-to-subword-tokenisers-4bbd3bad9a7c
tokens https://help.openai.com/en/articles/4936856-what-are-tokens-and-how-to-count-them
training (NN) -- Overfitting
training (NN) -- Reinforcement Learning
training (NN) -- supervised learning
training (NN) -- transfer learning = see fine-tuning
training (NN) -- unsupervised learning
training (NN) = iterative process in which data is passed to a neural network + parameters of the neural network are adjusted to achieve an optimal solution for the given problem. a dataset is required
training cutoff
training data -- illegal https://www.pcgamer.com/gaming-industry/court-documents-show-not-only-did-meta-torrent-terabytes-of-pirated-books-to-train-ai-models-employees-wouldnt-stop-emailing-each-other-about-it-torrenting-from-a-corporate-laptop-doesnt-feel-right/
Trainium
transfer learning
transformers "a new type of NLP model that demolished the reading comprehension abilities of both humans and the best AI incumbent at the time" ~pattern recognition techniques
transformers (hugging face) = a magic python library that can auto-download models on demand https://github.com/huggingface/transformers BUT it's for research, not prod (no unified API for ex.)
Turing test
vector -- difference with embedding??
vector database
vector database -- https://weaviate.io/
vector database -- https://www.pinecone.io
vector database -- Qdrant https://qdrant.tech/blog/series-a-funding-round/
vibe code fixers ;) https://vibecodefixers.com/
vibe coding -- Leo's example https://x.com/leojr94_/status/1903176192330477729
vibe coding https://x.com/karpathy/status/1886192184808149383
vs. low-code / no-code
vulnerability research https://googleprojectzero.blogspot.com/2024/10/from-naptime-to-big-sleep.html
watermarking outputs https://proceedings.mlr.press/v202/zhao23i.html
weak-to-strong generalization https://openai.com/research/weak-to-strong-generalization
who's who -- Karpathi
workload-model fit = a perfect alignment between what they desperately needed and what the AI delivers https://www.a16z.news/p/the-cinderella-glass-slipper-effect
yap -- yap score
yap https://www.reddit.com/r/ChatGPT/comments/1djgyvb/is_it_just_me_or_gpt4_yaps_too_much/
